---

information: |
  Hadoop cluster with 1 master and 3 nodes

parameters:

  locationId:
    information:
      - "the target data centre for this deployment"
    type: locations.list
    default: EU6

  domainName:
    information:
      - "the name of the network domain to be deployed"
    type: str
    default: HadoopClusterFox

  networkName:
    information:
      - "the name of the Ethernet VLAN to be deployed"
    type: str
    default: HadoopClusterNetwork

  cpuPerNode:
    information:
      - "the quantity of CPU given to one Hadoop node"
    type: [4..32]
    default: 8

  memoryPerNode:
    information:
      - "the quantity of RAM given to one Hadoop node, in GB"
    type: [8..256]
    default: 32

  diskPerNode:
    information:
      - "the quantity of storage given to one Hadoop node, in GB"
    type: [100..1000]
    default: 500

links:
  documentation: https://github.com/DimensionDataCBUSydney/plumbery-contrib/tree/master/fittings/analytics/hadoop-cluster
  credit: https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/ClusterSetup.html

defaults:

  # nodes are deployed in a single network domain
  domain:
    name: "{{ domainName.parameter }}"
    ipv4: auto

  # nodes are deployed on a single VLAN
  ethernet:
    name: "{{ networkName.parameter }}"
    subnet: 10.60.0.0

  # settings for any Hadoop master node
  hadoop-master:

    information:
      - "a Hadoop master node"
      - "connect remotely with:"
      - "$ ssh hadoop@{{ node.public }}"

    appliance: 'Ubuntu 14'

    cpu: 8
    memory: 32

    disks:
      - "1 100 standard"

    glue:
      - internet icmp 22

    monitoring: essentials

    cloud-config:

      runcmd:

        - echo "===== Preparing Hadoop naming space"
        - mkdir -p /home/hadoop/hdfs
        - hdfs namenode -format
        - chown -R hadoop:hadoop /home/hadoop

        - echo "===== Starting Hadoop NameNode"
        - hadoop-daemon.sh --script hdfs start namenode

        - echo "===== Starting Hadoop ResourceManager"
        - yarn-daemon.sh start resourcemanager

        - echo "===== Starting Hadoop NodeManager"
        - yarn-daemon.sh start nodemanager

        - echo "===== Testing Hadoop"
        - jps

        - echo "===== Checking HDFS status"
        - sleep 5m
        - hdfs dfsadmin -report

        - echo "===== Checking YARN status"
        - yarn node -list

  # settings for any Hadoop agent node
  hadoop-agent:

    appliance: 'Ubuntu 14'

    cpu: "{{ cpuPerNode.parameter }}"
    memory: "{{ memoryPerNode.parameter }}"

    disks:
      - "1 {{ diskPerNode.parameter }} standard"

    glue:
      - internet 22

    monitoring: essentials

    cloud-config:

      runcmd:

        - echo "===== Preparing Hadoop data space"
        - mkdir -p /home/hadoop/hdfs
        - hdfs namenode -format
        - chown -R hadoop:hadoop /home/hadoop

        - echo "===== Starting Hadoop DataNode"
        - hadoop-daemon.sh --script hdfs start datanode

        - echo "===== Testing Hadoop"
        - jps

  # directives that apply to all nodes
  cloud-config:

    hostname: "{{ node.name }}"

    write_files:

      - path: /root/hosts.awk
        content: |
          #!/usr/bin/awk -f
          /^{{ master1.private }}/ {next}
          /^{{ node1.private }}/ {next}
          /^{{ node2.private }}/ {next}
          /^{{ node3.private }}/ {next}
          {print}
          END {
           print "{{ master1.private }}    master1"
           print "{{ node1.private }}    node1"
           print "{{ node2.private }}    node2"
           print "{{ node3.private }}    node3"
          }

      - path: /etc/profile.d/hadoop.sh
        content: |
            export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
            export HADOOP_PREFIX=/home/hadoop/hadoop
            export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop
            export PATH=$PATH:$HADOOP_PREFIX/bin
            export PATH=$PATH:$HADOOP_PREFIX/sbin

      - path: /root/core-site.update
        content: |
          <property>
              <name>fs.defaultFS</name>
              <value>hdfs://{{ master1.private }}:9000</value>
          </property>

      - path: /root/hdfs-site.update
        content: |
          <property>
              <name>dfs.namenode.name.dir</name>
              <value>file:///home/hadoop/hdfs/</value>
              <description>NameNode directory for namespace and transaction logs storage.</description>
          </property>

          <property>
              <name>dfs.datanode.data.dir</name>
              <value>file:///home/hadoop/hdfs/</value>
              <description>DataNode directory for storing data chunks.</description>
          </property>

          <property>
              <name>dfs.replication</name>
              <value>3</value>
              <description>Number of replication for each chunk.</description>
          </property>

      - path: /root/yarn-site.update
        content: |
            <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>{{ master1.private }}</value>
                <description>The hostname of the ResourceManager</description>
            </property>

            <property>
              <name>yarn.nodemanager.aux-services</name>
              <value>mapreduce_shuffle</value>
              <description>shuffle service for MapReduce</description>
            </property>

      - path: /root/mapred-site.update
        content: |
            <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
                <description>Execution framework.</description>
            </property>

      - path: /home/hadoop/hadoop/etc/hadoop/slaves
        content: |
            node1
            node2
            node3

    bootcmd:

      - apt-get remove apache2 -y
      - apt-get autoremove -y

    packages:
      - ntp
      - openjdk-7-jdk
      - rsync

    runcmd:

      - echo "===== Growing LVM with added disk"
      - pvcreate /dev/sdb
      - vgextend rootvol00 /dev/sdb
      - lvextend -l +100%FREE /dev/mapper/rootvol00-rootlvol00
      - resize2fs /dev/mapper/rootvol00-rootlvol00

      - echo "===== Updating /etc/hosts"
      - cp -n /etc/hosts /etc/hosts.original
      - awk -f /root/hosts.awk /etc/hosts >/etc/hosts.new && mv /etc/hosts.new /etc/hosts
      - sed -i "/StrictHostKeyChecking/s/^.*$/    StrictHostKeyChecking no/" /etc/ssh/ssh_config

      - echo "===== Handling ubuntu identity"
      - cp -n /etc/ssh/ssh_host_rsa_key /home/ubuntu/.ssh/id_rsa
      - cp -n /etc/ssh/ssh_host_rsa_key.pub /home/ubuntu/.ssh/id_rsa.pub
      - chown ubuntu:ubuntu /home/ubuntu/.ssh/*

      - echo "===== Handling hadoop identity"
      - cp -n /etc/ssh/ssh_host_rsa_key /home/hadoop/.ssh/id_rsa
      - cp -n /etc/ssh/ssh_host_rsa_key.pub /home/hadoop/.ssh/id_rsa.pub
      - chown hadoop:hadoop /home/hadoop/.ssh/*

      - echo "===== Setting environment variables"
      - . /etc/profile.d/hadoop.sh
      - echo $PATH

      - echo "===== Checking Java"
      - java -version

      - echo "===== Installing Hadoop"
      - cd /tmp/
      - wget -q http://apache.trisect.eu/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz
      - tar -xzf hadoop-2.7.2.tar.gz
      - cp -R hadoop-2.7.2/* /home/hadoop/hadoop
      - chown -R hadoop:hadoop /home/hadoop

      - echo "===== Configuring Hadoop"
      - cd /home/hadoop/hadoop

      - cp -n etc/hadoop/core-site.xml etc/hadoop/core-site.xml.original
      - rm etc/hadoop/core-site.xml
      - cp etc/hadoop/core-site.xml.original etc/hadoop/core-site.xml
      - sed -i -e '/<configuration>/r /root/core-site.update' etc/hadoop/core-site.xml

      - cp -n etc/hadoop/hdfs-site.xml etc/hadoop/hdfs-site.xml.original
      - rm etc/hadoop/hdfs-site.xml
      - cp etc/hadoop/hdfs-site.xml.original etc/hadoop/hdfs-site.xml
      - sed -i -e '/<configuration>/r /root/hdfs-site.update' etc/hadoop/hdfs-site.xml

      - cp -n etc/hadoop/yarn-site.xml etc/hadoop/yarn-site.xml.original
      - rm etc/hadoop/yarn-site.xml
      - cp etc/hadoop/yarn-site.xml.original etc/hadoop/yarn-site.xml
      - sed -i -e '/<configuration>/r /root/yarn-site.update' etc/hadoop/yarn-site.xml

      - rm etc/hadoop/mapred-site.xml
      - cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml
      - sed -i -e '/<configuration>/r /root/mapred-site.update' etc/hadoop/mapred-site.xml

    ssh_keys:
      rsa_private: |
        {{ key.rsa_private }}
      rsa_public: "{{ key.rsa_public }}"

    users:
      - default

      - name: ubuntu
        sudo: 'ALL=(ALL) NOPASSWD:ALL'
        ssh-authorized-keys:
          - "{{ key.rsa_public }}"
          - "{{ local.rsa_public }}"

      - name: hadoop
        sudo: 'ALL=(ALL) NOPASSWD:ALL'
        ssh-authorized-keys:
          - "{{ key.rsa_public }}"
          - "{{ local.rsa_public }}"

    disable_root: true
    ssh_pwauth: false

---

locationId: "{{ locationId.parameter }}"

blueprints:

  - masters:

      nodes:
        - master1:
            default: hadoop-master

  - agents:

      nodes:
        - node[1..3]:
            default: hadoop-agent
